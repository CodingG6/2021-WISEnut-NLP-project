## 1주차 멘토링 회의 2차 기록

---

1주의 4일차부터는 데이터 사이즈를 늘려서(13,000건 이상) 분석을 해보고, 분석 속도(2-6)를 분석해볼 것

- 실무에서는 수학적 알고리즘 개발보다는 데이터 전처리에 훨씬 많은 시간이 소요됨
- 데이터 전처리 과정이 분석의 품질을 좌우함



### 데이터 전처리 프로그램 예시 정답을 보면서 토론

- 설정파일 **```config.yaml```**

  - 파일 형식은 자유. 꼭 ```yaml```을 써야 하는 것은 아님
    - 파이썬의 경우 ```.ini```로 가능
    - 그 외: .```json```, ```.xml``` 등 다양함

  

- 단어중요도 TF-IDF를 통한 단어별 점수 구하기: sklearn에서는 문서를 넣기만 하면 자동으로 계산해 줌. 이외에도 오픈소스 알고리즘이 많이 있음 

- 나중에 분석할 데이터 규모는 550,000건...!

- sklearn/gensim 등 문서가 너무 많으면 memory error발생

- 

### 메모리 효율

- 메모리 문제를 알고리즘과 설계를 통해 해결하는 것이 실무 능력을 키우는 지름길
- class 화 하기
- batch 작업: 문서양이 너무 많으면 문서를 100건/1000건 등 batch 단위로 읽도록 하면 메모리 효율을 높일 수 있음
- ```load_conf``` 의 ```conf variable```에 모든 변수 값이 다 들어 있음.
- ```Collections``` 사용이 ```dict.item 사용보다 효율적일 수 있음
- IDF = 동일 문서 내 등장 빈도수는 고려하지 않음. 오로지 등장한 문서 건수로 산정.